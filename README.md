# Auto Ski Info Subscribe - Social Media Crawler Service

è¿™æ˜¯ä¸€ä¸ªXçˆ¬è™«æœåŠ¡å™¨ï¼Œå®šæœŸçˆ¬å–Xï¼ˆTwitterï¼‰ã€å°çº¢ä¹¦ç­‰ç¤¾äº¤å¹³å°ä¸Šçš„å›ºå®šåšä¸»ä¿¡æ¯ï¼Œé€šè¿‡AI Deep Researchåå°†ç»“æœè¿”å›åˆ°æ¥å£ã€‚

This is a social media crawler service that periodically scrapes X (Twitter), Xiaohongshu (Little Red Book), and other social platforms for specific blogger information, processes the data through AI Deep Research, and returns results via API endpoints.

## Features

- ğŸ”„ **Periodic Crawling**: Automatically scrapes social media platforms at configurable intervals
- ğŸ¦ **Twitter/X Integration**: Uses Twitter API v2 for efficient data collection
- ğŸ“± **Xiaohongshu Support**: Crawler for Little Red Book platform
- ğŸ¤– **AI Deep Research**: Powered by OpenAI GPT-4 for comprehensive content analysis
- ğŸ—„ï¸ **MongoDB Storage**: Persistent storage for posts and research results
- ğŸš€ **RESTful API**: Easy-to-use API endpoints for accessing research results
- ğŸ“Š **Trend Analysis**: Generate trend reports from multiple posts
- â° **Scheduler**: Background task scheduler with configurable intervals

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flask API     â”‚  â† REST API endpoints
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scheduler     â”‚  â† Periodic task execution
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚Twitterâ”‚  â”‚XiaoHS â”‚  â† Platform crawlers
â”‚Crawlerâ”‚  â”‚Crawlerâ”‚
â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚        â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ MongoDB â”‚  â† Data storage
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ AI Research â”‚  â† OpenAI GPT-4
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### Prerequisites

- Python 3.8+
- MongoDB (local or cloud)
- OpenAI API key
- Twitter API credentials (Bearer Token or OAuth tokens)

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/cyohei9907/auto-ski-info-subscribe.git
cd auto-ski-info-subscribe
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your credentials
```

4. **Required environment variables:**
   - `OPENAI_API_KEY`: Your OpenAI API key
   - `TWITTER_BEARER_TOKEN`: Twitter API Bearer Token
   - `MONGODB_URI`: MongoDB connection string
   - `TWITTER_TARGET_USERS`: Comma-separated list of Twitter usernames to monitor
   - `XIAOHONGSHU_TARGET_USERS`: Comma-separated list of Xiaohongshu usernames

## Usage

### Start the service

```bash
python app.py
```

The service will:
1. Start the Flask API server (default: http://localhost:5000)
2. Initialize the scheduler for periodic crawling
3. Begin monitoring configured social media accounts

### API Endpoints

#### Get Service Info
```bash
GET /
```

#### Health Check
```bash
GET /health
```

#### Get Latest Research Results
```bash
GET /api/research/latest?limit=50
```

#### Get Research by Platform
```bash
GET /api/research/platform/Twitter?limit=50
GET /api/research/platform/Xiaohongshu?limit=50
```

#### Get Research by User
```bash
GET /api/research/user/username?limit=50
```

#### Manually Trigger Crawl
```bash
POST /api/crawl/twitter
POST /api/crawl/xiaohongshu
```

#### Analyze Custom Post
```bash
POST /api/analyze
Content-Type: application/json

{
  "text": "Your post content here",
  "platform": "Twitter"
}
```

### Example Response

```json
{
  "success": true,
  "count": 10,
  "results": [
    {
      "_id": "...",
      "post_id": "...",
      "platform": "Twitter",
      "analysis": "Detailed AI analysis...",
      "summary": "Brief summary...",
      "key_points": [
        "Point 1",
        "Point 2"
      ],
      "sentiment": "positive",
      "created_at": "2024-01-01T00:00:00"
    }
  ]
}
```

## Configuration

Edit `.env` file to customize:

- **Crawling Interval**: `SCRAPE_INTERVAL_HOURS` (default: 6 hours)
- **API Port**: `API_PORT` (default: 5000)
- **OpenAI Model**: `OPENAI_MODEL` (default: gpt-4)
- **Target Users**: Add usernames to monitor
- **Enable/Disable Scheduler**: `ENABLE_SCHEDULER` (true/false)

## Project Structure

```
auto-ski-info-subscribe/
â”œâ”€â”€ app.py                 # Flask API server
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ scheduler.py           # Task scheduler
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_crawler.py   # Base crawler class
â”‚   â”œâ”€â”€ twitter_crawler.py     # Twitter implementation
â”‚   â””â”€â”€ xiaohongshu_crawler.py # Xiaohongshu implementation
â”œâ”€â”€ ai_research/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ deep_research.py  # AI analysis module
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ logger.py         # Logging utilities
    â””â”€â”€ database.py       # MongoDB management
```

## Development

### Add New Platform

1. Create new crawler in `crawlers/` extending `BaseCrawler`
2. Implement `authenticate()` and `get_user_posts()` methods
3. Add platform configuration in `config.py`
4. Register crawler in scheduler

### Run Tests

```bash
# Tests can be added here
pytest
```

## Deployment

### Docker (Recommended)

```bash
# Build image
docker build -t ski-info-crawler .

# Run container
docker run -d \
  --env-file .env \
  -p 5000:5000 \
  --name crawler \
  ski-info-crawler
```

### Production Considerations

- Use production-grade WSGI server (Gunicorn, uWSGI)
- Set up reverse proxy (Nginx)
- Configure proper logging
- Implement rate limiting
- Set up monitoring and alerts
- Use environment-specific configurations

## Security

- Never commit `.env` file or API credentials
- Use environment variables for sensitive data
- Implement API authentication for production
- Follow platform API rate limits and terms of service
- Regularly update dependencies

## Troubleshooting

### MongoDB Connection Issues
- Verify MongoDB is running
- Check connection string in `.env`
- Ensure network access to MongoDB server

### Twitter API Errors
- Verify API credentials
- Check rate limits
- Ensure proper API access level

### OpenAI API Errors
- Verify API key is valid
- Check account balance/credits
- Monitor token usage

## License

MIT License

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## Support

For issues and questions:
- Open an issue on GitHub
- Check existing documentation
- Review logs in `logs/` directory

## Roadmap

- [ ] Support for more social platforms (Instagram, TikTok, etc.)
- [ ] Advanced filtering and search capabilities
- [ ] Real-time notifications for new posts
- [ ] Web dashboard for visualization
- [ ] Sentiment trend analysis
- [ ] Multi-language support
- [ ] Export to various formats (CSV, Excel, PDF)

---

Built with â¤ï¸ for social media intelligence and research