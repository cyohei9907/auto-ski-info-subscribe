import logging
import re
import asyncio
import random
import time
from typing import List, Optional, Dict
from datetime import datetime, timezone
from django.conf import settings
from django.utils import timezone as django_timezone
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
from .models import XAccount, Tweet, MonitoringLog

logger = logging.getLogger(__name__)

# ä¸´æ—¶ä½¿ç”¨workaround scraperï¼ˆå› ä¸ºauthenticated_scraperè¢«X.comæ£€æµ‹ï¼‰
USE_WORKAROUND = True  # ä¸´æ—¶å¯ç”¨workaround
if USE_WORKAROUND:
    logger.info("ğŸ”§ Using workaround scraper (temporary fix for X.com anti-automation)")
    from .workaround_scraper import scrape_with_working_method
    SCRAPER_AVAILABLE = True
else:
    # æ ¹æ®é…ç½®é€‰æ‹©çˆ¬è™«å®ç°
    USE_AUTHENTICATED = getattr(settings, 'USE_AUTHENTICATED_SCRAPER', False)
    if USE_AUTHENTICATED:
        logger.info("Using authenticated X.com scraper (requires cookies, can access full timeline)")
        try:
            from .authenticated_scraper import AuthenticatedXScraperClient
            SCRAPER_AVAILABLE = True
        except ImportError as e:
            logger.warning(f"Failed to import authenticated scraper: {e}, falling back to guest scraper")
            SCRAPER_AVAILABLE = False
    else:
        logger.info("Using guest X.com scraper (limited to visible tweets)")
        SCRAPER_AVAILABLE = False



class XScraperClient:
    """X (Twitter) Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.base_url = "https://x.com"  # ä½¿ç”¨æ–°åŸŸå x.com
    
    def _extract_tweet_id(self, tweet_url: str) -> Optional[str]:
        """ãƒ„ã‚¤ãƒ¼ãƒˆURLã‹ã‚‰IDã‚’æŠ½å‡º"""
        match = re.search(r'/status/(\d+)', tweet_url)
        return match.group(1) if match else None
    
    def _parse_tweet_time(self, time_str: str) -> datetime:
        """ç›¸å¯¾æ™‚é–“ã‚’çµ¶å¯¾æ™‚é–“ã«å¤‰æ›"""
        now = django_timezone.now()
        
        # "2æ™‚é–“" -> 2æ™‚é–“å‰
        if 'æ™‚é–“' in time_str or 'h' in time_str:
            hours = int(re.search(r'\d+', time_str).group())
            return now - django_timezone.timedelta(hours=hours)
        # "15åˆ†" -> 15åˆ†å‰
        elif 'åˆ†' in time_str or 'm' in time_str:
            minutes = int(re.search(r'\d+', time_str).group())
            return now - django_timezone.timedelta(minutes=minutes)
        # "30ç§’" -> 30ç§’å‰
        elif 'ç§’' in time_str or 's' in time_str:
            seconds = int(re.search(r'\d+', time_str).group())
            return now - django_timezone.timedelta(seconds=seconds)
        # "3æ—¥" -> 3æ—¥å‰
        elif 'æ—¥' in time_str or 'd' in time_str:
            days = int(re.search(r'\d+', time_str).group())
            return now - django_timezone.timedelta(days=days)
        else:
            return now
    
    def _add_random_delay(self):
        """æ·»åŠ éšæœºå»¶è¿Ÿä»¥é¿å…è¢«æ£€æµ‹ä¸ºæœºå™¨äºº (15-30ç§’)"""
        delay = random.uniform(15, 30)
        logger.info(f"Waiting {delay:.1f} seconds before next request...")
        time.sleep(delay)
    
    def get_user_by_username(self, username: str) -> Optional[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            self._add_random_delay()
            
            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox'
                    ]
                )
                context = browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9,ja;q=0.8',
                    }
                )
                page = context.new_page()
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                url = f"{self.base_url}/{username}"
                logger.info(f"Navigating to user profile: {url}")
                
                try:
                    page.goto(url, wait_until='domcontentloaded', timeout=60000)
                    # ç­‰å¾…ç”¨æˆ·ä¿¡æ¯åŠ è½½
                    page.wait_for_selector('[data-testid="UserName"]', timeout=20000)
                except PlaywrightTimeoutError as e:
                    logger.error(f"Timeout loading user profile: {e}")
                    browser.close()
                    return None
                
                # éšæœºç­‰å¾… 2-5 ç§’
                wait_time = random.uniform(2000, 5000)
                page.wait_for_timeout(int(wait_time))
                
                # HTMLã‚’å–å¾—
                html = page.content()
                soup = BeautifulSoup(html, 'lxml')
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å–å¾—ï¼ˆdisplayNameï¼‰
                display_name = username
                try:
                    # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–æ˜¾ç¤ºåç§°
                    name_elem = soup.select_one('[data-testid="UserName"] span') or \
                               soup.select_one('[data-testid="UserProfileHeader_Items"] span')
                    if name_elem:
                        # è·å–ç¬¬ä¸€ä¸ªspançš„æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯display nameï¼‰
                        display_name = name_elem.get_text().strip()
                except Exception as e:
                    logger.warning(f"Failed to extract display name: {e}")
                
                # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒã‚’å–å¾—ï¼ˆå¤šç§æ–¹æ³•å°è¯•ï¼‰
                avatar_url = None
                try:
                    # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«profile_imagesçš„imgæ ‡ç­¾
                    avatar_elem = soup.find('img', src=re.compile(r'profile_images'))
                    if avatar_elem and 'src' in avatar_elem.attrs:
                        avatar_url = avatar_elem['src']
                        # è·å–æ›´é«˜æ¸…ç‰ˆæœ¬ï¼ˆå°†_normalæ›¿æ¢ä¸º_400x400ï¼‰
                        avatar_url = avatar_url.replace('_normal', '_400x400')
                    
                    # æ–¹æ³•2: æŸ¥æ‰¾data-testid="UserAvatar-Container"ä¸‹çš„img
                    if not avatar_url:
                        avatar_container = soup.find('div', {'data-testid': 'UserAvatar-Container-'}) or \
                                         soup.find('div', class_=re.compile(r'css.*Avatar'))
                        if avatar_container:
                            avatar_elem = avatar_container.find('img')
                            if avatar_elem and 'src' in avatar_elem.attrs:
                                avatar_url = avatar_elem['src'].replace('_normal', '_400x400')
                    
                    # æ–¹æ³•3: å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å¤´åƒ
                    if not avatar_url:
                        avatar_url = 'https://abs.twimg.com/sticky/default_profile_images/default_profile_400x400.png'
                        
                except Exception as e:
                    logger.warning(f"Failed to extract avatar: {e}")
                    avatar_url = 'https://abs.twimg.com/sticky/default_profile_images/default_profile_400x400.png'
                
                browser.close()
                
                logger.info(f"User info scraped - username: {username}, display_name: {display_name}, avatar: {avatar_url}")
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ç”Ÿæˆ(ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—ã§ããªã„ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’IDã¨ã—ã¦ä½¿ç”¨)
                return {
                    'id': username,  # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’IDã¨ã—ã¦ä½¿ç”¨
                    'username': username,
                    'name': display_name,
                    'profile_image_url': avatar_url
                }
                
        except Exception as e:
            logger.error(f"Error scraping user {username}: {e}")
            return None
    
    def get_recent_tweets(self, username: str, max_results: int = 10, hours: int = 6) -> List[Dict]:
        """æœ€æ–°ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæŒ‡å®šæ™‚é–“ä»¥å†…ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿ï¼‰
        
        Args:
            username: X.comç”¨æˆ·å
            max_results: æœ€å¤§å–å¾—ãƒ„ã‚¤ãƒ¼ãƒˆæ•°
            hours: æ™‚é–“ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 6æ™‚é–“ï¼‰
        """
        try:
            # å¦‚æœå¯ç”¨äº†workaroundï¼Œä½¿ç”¨å®ƒ
            if USE_WORKAROUND:
                logger.info(f"ä½¿ç”¨workaround scraperè·å– @{username} çš„æ¨æ–‡")
                tweets = scrape_with_working_method(username, max_tweets=max_results)
                
                # è¿‡æ»¤æŒ‡å®šæ—¶é—´å†…çš„æ¨æ–‡
                time_ago = django_timezone.now() - django_timezone.timedelta(hours=hours)
                recent_tweets = []
                for tweet in tweets:
                    try:
                        tweet_time = django_timezone.datetime.fromisoformat(tweet['published_at'].replace('Z', '+00:00'))
                        if tweet_time >= time_ago:
                            recent_tweets.append(tweet)
                    except:
                        pass
                
                logger.info(f"Workaround scraperæ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡ï¼Œå…¶ä¸­ {len(recent_tweets)} æ¡åœ¨{hours}å°æ—¶å†…")
                return recent_tweets
            
            # åŸæœ‰é€»è¾‘...
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            self._add_random_delay()
            
            # è®¡ç®—6å°æ—¶å‰çš„æ—¶é—´
            six_hours_ago = django_timezone.now() - django_timezone.timedelta(hours=6)
            logger.info(f"Fetching tweets since: {six_hours_ago}")
            
            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-gpu',
                        '--disable-software-rasterizer'
                    ]
                )
                context = browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9,ja;q=0.8',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    }
                )
                
                # ç¦ç”¨å­—ä½“ã€æ ·å¼è¡¨ç­‰éå¿…è¦èµ„æºä»¥åŠ å¿«åŠ è½½é€Ÿåº¦ï¼ˆä¿ç•™å›¾ç‰‡å’Œè§†é¢‘ï¼‰
                def block_resources(route):
                    resource_type = route.request.resource_type
                    if resource_type in ['font', 'stylesheet']:
                        route.abort()
                    else:
                        route.continue_()
                
                context.route("**/*", block_resources)
                page = context.new_page()
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹
                url = f"{self.base_url}/{username}"
                logger.info(f"Navigating to {url}")
                
                try:
                    # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶å’Œæ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
                    page.goto(url, wait_until='domcontentloaded', timeout=60000)
                    # ç­‰å¾…æ¨æ–‡åŠ è½½
                    page.wait_for_selector('article[data-testid="tweet"]', timeout=30000)
                except PlaywrightTimeoutError as e:
                    logger.error(f"Timeout waiting for page or tweets: {e}")
                    # ä¿å­˜æˆªå›¾ç”¨äºè°ƒè¯•
                    screenshot_path = f"/tmp/x_scrape_error_{username}.png"
                    page.screenshot(path=screenshot_path)
                    logger.error(f"Screenshot saved to {screenshot_path}")
                    browser.close()
                    return []
                
                # æœ€å°åŒ–æ»šåŠ¨ï¼šåªæ»šåŠ¨ä¸€æ¬¡ï¼Œå¿«é€Ÿè·å–æœ€æ–°æ¨æ–‡
                page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # çŸ­æš‚ç­‰å¾…å†…å®¹åŠ è½½
                wait_time = random.uniform(1000, 2000)
                page.wait_for_timeout(int(wait_time))
                
                # HTMLã‚’å–å¾—
                html = page.content()
                soup = BeautifulSoup(html, 'lxml')
                
                # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’è§£æ
                tweets = []
                tweet_articles = soup.find_all('article', {'data-testid': 'tweet'})
                logger.info(f"Found {len(tweet_articles)} tweet articles on page")
                
                # ç»Ÿè®¡ï¼šå‘ç°äº†å¤šå°‘6å°æ—¶å†…çš„æ¨æ–‡
                recent_count = 0
                old_count = 0
                
                if len(tweet_articles) == 0:
                    # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                    with open(f'/tmp/x_page_{username}.html', 'w', encoding='utf-8') as f:
                        f.write(html)
                    logger.warning(f"No tweets found. HTML saved to /tmp/x_page_{username}.html")
                
                for article in tweet_articles[:max_results]:
                    try:
                        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
                        tweet_text_elem = article.find('div', {'data-testid': 'tweetText'})
                        tweet_text = tweet_text_elem.get_text() if tweet_text_elem else ""
                        
                        # ãƒ„ã‚¤ãƒ¼ãƒˆID (URLã‹ã‚‰å–å¾—)
                        tweet_link = article.find('a', href=re.compile(r'/status/\d+'))
                        tweet_id = None
                        if tweet_link and 'href' in tweet_link.attrs:
                            tweet_id = self._extract_tweet_id(tweet_link['href'])
                        
                        if not tweet_id:
                            continue
                        
                        # æŠ•ç¨¿æ™‚é–“
                        time_elem = article.find('time')
                        posted_at = None
                        if time_elem and 'datetime' in time_elem.attrs:
                            posted_at = datetime.fromisoformat(time_elem['datetime'].replace('Z', '+00:00'))
                        else:
                            # ç›¸å¯¾æ™‚é–“ã‹ã‚‰æ¨å®š
                            time_text = time_elem.get_text() if time_elem else ""
                            posted_at = self._parse_tweet_time(time_text)
                        
                        # 6æ™‚é–“ä»¥å†…ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿å‡¦ç†
                        if posted_at and posted_at < six_hours_ago:
                            old_count += 1
                            logger.info(f"Tweet {tweet_id} is older than 6 hours ({posted_at}), skipping")
                            continue
                        
                        recent_count += 1
                        
                        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¨ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
                        hashtags = [tag.get_text()[1:] for tag in article.find_all('a', href=re.compile(r'/hashtag/'))]
                        mentions = [mention.get_text()[1:] for mention in article.find_all('a', href=re.compile(r'/[^/]+$')) if mention.get_text().startswith('@')]
                        
                        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™
                        reply_count = 0
                        retweet_count = 0
                        like_count = 0
                        
                        # è¿”ä¿¡æ•°
                        reply_elem = article.find('button', {'data-testid': 'reply'})
                        if reply_elem:
                            reply_text = reply_elem.get_text()
                            reply_match = re.search(r'\d+', reply_text)
                            reply_count = int(reply_match.group()) if reply_match else 0
                        
                        # ãƒªãƒ„ã‚¤ãƒ¼ãƒˆæ•°
                        retweet_elem = article.find('button', {'data-testid': 'retweet'})
                        if retweet_elem:
                            retweet_text = retweet_elem.get_text()
                            retweet_match = re.search(r'\d+', retweet_text)
                            retweet_count = int(retweet_match.group()) if retweet_match else 0
                        
                        # ã„ã„ã­æ•°
                        like_elem = article.find('button', {'data-testid': 'like'})
                        if like_elem:
                            like_text = like_elem.get_text()
                            like_match = re.search(r'\d+', like_text)
                            like_count = int(like_match.group()) if like_match else 0
                        
                        tweet_data = {
                            'id': tweet_id,
                            'text': tweet_text,
                            'created_at': posted_at,
                            'retweet_count': retweet_count,
                            'like_count': like_count,
                            'reply_count': reply_count,
                            'hashtags': hashtags,
                            'mentions': mentions,
                            'media_urls': []
                        }
                        tweets.append(tweet_data)
                        logger.info(f"Parsed tweet {tweet_id}: {tweet_text[:50]}...")
                        
                    except Exception as e:
                        logger.warning(f"Error parsing individual tweet: {e}")
                        continue
                
                browser.close()
                logger.info(f"Successfully scraped {len(tweets)} tweets for @{username} (recent: {recent_count}, old: {old_count})")
                return tweets
                
        except Exception as e:
            logger.error(f"Error scraping tweets for user {username}: {e}")
            return []
    
    def get_today_tweets(self, username: str) -> List[Dict]:
        """å½“æ—¥ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿ã‚’å–å¾—ï¼ˆ24å°æ™‚ä»¥å†…ï¼‰"""
        try:
            # å¦‚æœå¯ç”¨äº†workaroundï¼Œä½¿ç”¨å®ƒè·å–æ›´å¤šæ¨æ–‡
            if USE_WORKAROUND:
                logger.info(f"ä½¿ç”¨workaround scraperè·å– @{username} å½“æ—¥æ¨æ–‡")
                tweets = scrape_with_working_method(username, max_tweets=50)  # è·å–æ›´å¤šæ¨æ–‡
                
                # è¿‡æ»¤24å°æ—¶å†…çš„æ¨æ–‡
                twenty_four_hours_ago = django_timezone.now() - django_timezone.timedelta(hours=24)
                today_tweets = []
                for tweet in tweets:
                    try:
                        tweet_time = django_timezone.datetime.fromisoformat(tweet['published_at'].replace('Z', '+00:00'))
                        if tweet_time >= twenty_four_hours_ago:
                            today_tweets.append(tweet)
                    except:
                        pass
                
                logger.info(f"Workaround scraperæ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡ï¼Œå…¶ä¸­ {len(today_tweets)} æ¡åœ¨24å°æ—¶å†…")
                return today_tweets
            
            # åŸæœ‰é€»è¾‘ï¼ˆä½œä¸ºåå¤‡ï¼‰
            all_tweets = self.get_recent_tweets(username, max_results=50)
            
            # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
            today = django_timezone.now().date()
            
            # å½“æ—¥ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            today_tweets = [
                tweet for tweet in all_tweets 
                if tweet['created_at'].date() == today
            ]
            
            logger.info(f"Found {len(today_tweets)} tweets today for @{username}")
            return today_tweets
            
        except Exception as e:
            logger.error(f"Error getting today's tweets for user {username}: {e}")
            return []


class XMonitorService:
    """Xç›£è¦–ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        # æ ¹æ®é…ç½®é€‰æ‹©çˆ¬è™«å®ç°
        if USE_WORKAROUND:
            self.scraper_client = XScraperClient()  # ä½¿ç”¨åŸºç¡€clientï¼Œä½†get_recent_tweetsä¼šè°ƒç”¨workaround
            logger.info("XMonitorService initialized with workaround scraper")
        elif 'USE_AUTHENTICATED' in globals() and USE_AUTHENTICATED and SCRAPER_AVAILABLE:
            self.scraper_client = AuthenticatedXScraperClient()
            logger.info("XMonitorService initialized with authenticated scraper")
        else:
            self.scraper_client = XScraperClient()
            logger.info("XMonitorService initialized with guest scraper")
    
    def monitor_account(self, x_account: XAccount, today_only: bool = False, max_tweets: int = 20, hours: int = 6) -> dict:
        """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ç›£è¦–ã—ã¦æ–°ã—ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—
        
        Args:
            x_account: ç›£è¦–ã™ã‚‹Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
            today_only: Trueã®å ´åˆã€å½“æ—¥ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿ã‚’å–å¾—
            max_tweets: å–å¾—ã™ã‚‹æœ€å¤§ãƒ„ã‚¤ãƒ¼ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰
            hours: æ™‚é–“ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 6æ™‚é–“ã€today_onlyãŒFalseã®å ´åˆã®ã¿æœ‰åŠ¹ï¼‰
        """
        start_time = django_timezone.now()
        
        try:
            # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—
            if today_only:
                tweets_data = self.scraper_client.get_today_tweets(
                    username=x_account.username
                )
            else:
                tweets_data = self.scraper_client.get_recent_tweets(
                    username=x_account.username,
                    max_results=max_tweets,
                    hours=hours
                )
            
            new_tweets_count = 0
            
            # ä¸å†ä»æ¨æ–‡ä¸­æ›´æ–°è´¦æˆ·å¤´åƒ
            # å¤´åƒåº”è¯¥åªåœ¨é¦–æ¬¡æ·»åŠ è´¦æˆ·æ—¶ä»ç”¨æˆ·èµ„æ–™é¡µè·å–ï¼Œä¹‹åä¸å†å˜æ›´
            
            # æ–°ã—ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            for tweet_data in tweets_data:
                if not Tweet.objects.filter(tweet_id=tweet_data['id']).exists():
                    Tweet.objects.create(
                        x_account=x_account,
                        tweet_id=tweet_data['id'],
                        content=tweet_data['text'],
                        hashtags=tweet_data['hashtags'],
                        mentions=tweet_data['mentions'],
                        media_urls=tweet_data['media_urls'],
                        retweet_count=tweet_data['retweet_count'],
                        like_count=tweet_data['like_count'],
                        reply_count=tweet_data['reply_count'],
                        posted_at=tweet_data['created_at']
                    )
                    new_tweets_count += 1
            
            # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯æ™‚åˆ»ã‚’æ›´æ–°
            x_account.last_checked = django_timezone.now()
            x_account.save()
            
            # ãƒ­ã‚°ã‚’è¨˜éŒ²
            execution_time = (django_timezone.now() - start_time).total_seconds()
            log_result = 'success' if new_tweets_count > 0 else 'no_new_tweets'
            
            MonitoringLog.objects.create(
                x_account=x_account,
                result=log_result,
                tweets_found=new_tweets_count,
                execution_time=execution_time
            )
            
            return {
                'success': True,
                'new_tweets': new_tweets_count,
                'execution_time': execution_time
            }
            
        except Exception as e:
            execution_time = (django_timezone.now() - start_time).total_seconds()
            
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è¨˜éŒ²
            MonitoringLog.objects.create(
                x_account=x_account,
                result='error',
                tweets_found=0,
                error_message=str(e),
                execution_time=execution_time
            )
            
            logger.error(f"Error monitoring account @{x_account.username}: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'execution_time': execution_time
            }
    
    def setup_account_monitoring(self, username: str) -> Optional[dict]:
        """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            user_info = self.scraper_client.get_user_by_username(username)
            if not user_info:
                return None
                
            return {
                'user_id': user_info['id'],
                'username': user_info['username'],
                'display_name': user_info['name'],
                'avatar_url': user_info['profile_image_url']
            }
        except Exception as e:
            logger.error(f"Error setting up monitoring for @{username}: {e}")
            return None